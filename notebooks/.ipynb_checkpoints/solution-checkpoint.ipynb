{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Unconditional generation.\n",
    "\n",
    "def generate_unconditionally(random_seed=1):\n",
    "    # Input:\n",
    "    #   random_seed - integer\n",
    "\n",
    "    # Output:\n",
    "    #   stroke - numpy 2D-array (T x 3)\n",
    "    return stroke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import statements\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Dataset\n",
    "strokes = np.load('../data/strokes-py3.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 0.8\n",
    "val_split = 0.1\n",
    "# Maybe shuffle the dataset with a fixed random seed\n",
    "train_set = np.array(sorted(strokes[:int(train_split*len(strokes))], key=len, reverse=True))\n",
    "val_set = np.array(sorted(strokes[int(train_split*len(strokes)):int((train_split+val_split)*len(strokes))], key=len, reverse=True))\n",
    "test_set = np.array(sorted(strokes[int((train_split+val_split)*len(strokes)):], key=len, reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 4800\n",
      "Validation set: 600\n",
      "Testing set: 600\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set: {}\\nValidation set: {}\\nTesting set: {}\".format(len(train_set), len(val_set), len(test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize x and y coordinates to 0 mean 1 std\n",
    "def normalize(data):\n",
    "    data_concat = np.concatenate(data, axis=0)\n",
    "    means = np.mean(data_concat, axis=0)\n",
    "    stds = np.std(data_concat, axis=0)\n",
    "    x_mean = means[1]\n",
    "    y_mean = means[2]\n",
    "    x_std = stds[1]\n",
    "    y_std = stds[2]\n",
    "    for element in data:\n",
    "        element[:,1] = (element[:,1] - x_mean) / x_std\n",
    "        element[:,2] = (element[:,2] - y_mean) / y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize(train_set)\n",
    "normalize(val_set)\n",
    "normalize(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pack sequences\n",
    "dataset = [train_set, val_set, test_set]\n",
    "\n",
    "for k in range(len(dataset)):\n",
    "    dataset[k] = [torch.from_numpy(x) for x in dataset[k]]\n",
    "#     dataset[k] = rnn_utils.pack_sequence(dataset[k])\n",
    "train_set, val_set, test_set = tuple(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = train_set[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed = rnn_utils.pack_sequence(sample_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpack, batch_len = rnn_utils.pad_packed_sequence(packed, padding_value=np.inf, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unpack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1166, 3])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpack[0][:batch_len[9]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1191, 1189, 1184, 1181, 1180, 1175, 1173, 1173, 1170, 1166])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpacked = []\n",
    "for k in range(len(unpack)):\n",
    "    unpacked.append(unpack[k][:batch_len[k]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class lstm(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers, output_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, dropout=0.2)\n",
    "        self.out_layer = nn.Linear(hidden_size, output_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "    \n",
    "    def forward(self, x, h):\n",
    "        x = rnn_utils.pack_sequence(x)\n",
    "        x, h = self.lstm(x, h)\n",
    "        x, batch_len = rnn_utils.pad_packed_sequence(x, padding_value=np.inf, batch_first=True)\n",
    "        unpacked = []\n",
    "        for k in range(len(x)):\n",
    "            unpacked.append(x[k][:batch_len[k]])\n",
    "        x = torch.cat(unpacked).contiguous().view(-1, self.hidden_size)\n",
    "        x = self.out_layer(x)\n",
    "        return x, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "    \n",
    "    def dataloader(self, dataset, batch_size):\n",
    "        n = len(dataset)\n",
    "    #     assert batch_size <= n\n",
    "    #         raise AssertionError('batch_size must be less than dataset size')\n",
    "        i = 0\n",
    "        while i < n:\n",
    "            if i + batch_size <= n:\n",
    "                yield dataset[i:i+batch_size]\n",
    "                i += batch_size\n",
    "            else:\n",
    "                yield dataset[i:]\n",
    "                i = n\n",
    "                \n",
    "    def save_model(self, model_params_dir, filename, epoch):\n",
    "        if not os.path.isdir(model_params_dir):\n",
    "            os.makedirs(model_params_dir)\n",
    "            \n",
    "        torch.save(\n",
    "            {\"epoch\": epoch, \"model_state_dict\": model.state_dict()},\n",
    "            os.path.join(model_params_dir, filename),\n",
    "        )\n",
    "        \n",
    "    def save_logs(self, logs_directory, loss_log, epoch):\n",
    "        \n",
    "        torch.save(\n",
    "            { \"epoch\": epoch, \"loss\": loss_log, \"val_loss\": val_loss},\n",
    "            os.path.join(logs_directory, ws.logs_filename),\n",
    "        )\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = lstm(3, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, batch_len = lstm_model(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4800, 1191, 3])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4410, -0.2608,  0.0131], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0,1190]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "def loss_function(y_pred, y_true):\n",
    "    lift_true = y_true[:, 0]\n",
    "    lift_pred = y_pred[:, 0]\n",
    "    lift_loss = nn.BCEWithLogitsLoss()(lift_pred, lift_true)\n",
    "    l1_loss = nn.L1Loss()(y_pred[:, 1:] , y_true[:, 1:])\n",
    "    loss = lift_loss + l1_loss\n",
    "    return loss\n",
    "    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_set, val_set, epochs, batch_size):\n",
    "    counter = 0\n",
    "    print_every = 100\n",
    "    clip = 5\n",
    "    loss_log = []\n",
    "    val_loss_log = []\n",
    "    for epoch in range(1, epochs+1):\n",
    "        print(\"epoch: \", epoch)\n",
    "        # initialize hidden state\n",
    "        model.train()\n",
    "        h = model.init_hidden(batch_size)\n",
    "\n",
    "        # Training batch loop\n",
    "        for inputs in model.dataloader(train_set, batch_size):\n",
    "            print(counter)\n",
    "            labels = [torch.zeros_like(k) for k in inputs]\n",
    "            for i in range(len(labels)):\n",
    "                labels[i][:-1, :] = inputs[i][1:, :]\n",
    "                labels[i][-1, :] = inputs[i][0, :]\n",
    "            labels = torch.cat(labels)\n",
    "            counter += 1\n",
    "\n",
    "            # if(train_on_gpu):\n",
    "            #     inputs = inputs.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # get the output from the model\n",
    "            output, h = model(inputs, h)\n",
    "\n",
    "            # calculate the loss and perform backprop\n",
    "            loss = loss_function(output.squeeze(), labels)\n",
    "            loss_log.append(loss.item())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "        # Get validation loss\n",
    "        val_h = model.init_hidden(batch_size)\n",
    "        model.eval()\n",
    "        for val_inputs in model.dataloader(val_set, batch_size):\n",
    "            val_labels = [torch.zeros_like(k) for k in val_inputs]\n",
    "            for i in range(len(val_labels)):\n",
    "                val_labels[i][:-1, :] = val_inputs[i][1:, :]\n",
    "                val_labels[i][-1, :] = val_inputs[i][0, :]\n",
    "            val_labels = torch.cat(val_labels)\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "            # if(train_on_gpu):\n",
    "            #     val_input = val_input.cuda()\n",
    "\n",
    "            val_output, val_h = model(val_inputs, val_h)\n",
    "            val_loss = loss_function(val_output.squeeze(), val_labels)\n",
    "\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "        model.train()\n",
    "        print(\"Epoch: {}/{}...\".format(epoch, epochs),\n",
    "              \"Step: {}...\".format(counter),\n",
    "              \"Loss: {:.6f}...\".format(loss.item()),\n",
    "              \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "\n",
    "        if epoch in checkpoints:\n",
    "            model.save_model(epoch, str(epoch)+\".pth\")\n",
    "\n",
    "        if epoch % log_frequency == 0:\n",
    "            model.save_model(epoch, \"latest.pth\")\n",
    "            model.save_logs(\n",
    "                logs_directory,\n",
    "                loss_log,\n",
    "                val_loss_log,\n",
    "                epoch,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.workspace as ws\n",
    "from utils.workspace import normalize\n",
    "from models.generate_unconditionnally import LSTM as lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_directory = \"../unconditional_generation/\"\n",
    "specs = ws.load_experiment_specifications(experiment_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_specs = specs[\"NetworkSpecs\"]\n",
    "# input_dim = net_specs[\"InputDim\"]\n",
    "# hidden_dim = net_specs[\"HiddenDim\"]\n",
    "# n_layers = net_specs[\"NumLayersLSTM\"]\n",
    "# output_dim = net_specs[\"OutputDim\"]\n",
    "# model = lstm(input_dim=input_dim, hidden_dim=hidden_dim, n_layers=n_layers, output_dim=output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_model_parameters(os.path.join(\"..\",specs['ModelDir']), \"100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, '../models/ugm.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../models/ugm.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-17-d88145d972c4>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-d88145d972c4>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def get_stroke(m):\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def get_stroke(m):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "hidden = model.init_hidden(batch_size)\n",
    "#Randomize the first stroke with random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]),\n",
       " tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "point = torch.Tensor([[[1,0,0]]])\n",
    "np.random.seed(1)\n",
    "stroke_length = np.random.randint(specs[\"StrokeMinLength\"], specs[\"StrokeMaxLength\"]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stroke_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "point, hidden = model(point,hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7169,  0.8625,  1.1118]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "point[:, 0] = torch.sigmoid(point[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3281, 0.8625, 1.1118]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "point[:, 0] = torch.Tensor([1 if x > 0.5 else 0 for x in point[:, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.8625, 1.1118]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy\n",
    "import matplotlib\n",
    "import sys \n",
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "\n",
    "sys.path.insert(0,'..')\n",
    "from utils import plot_stroke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stroke(model):\n",
    "    model.eval()\n",
    "    stroke_length = np.random.randint(specs[\"StrokeMinLength\"], specs[\"StrokeMaxLength\"]+1)\n",
    "    batch_size = 1\n",
    "    stroke = torch.zeros(stroke_length, batch_size, 3)\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    point = torch.Tensor([[[1,0,0]]])\n",
    "    \n",
    "    # Randomize Length of stroke with random seed\n",
    "    for k in range(stroke_length):\n",
    "        model.eval()\n",
    "\n",
    "        point, hidden = model(point,hidden)\n",
    "        point[:, 0] = torch.sigmoid(point[:, 0])\n",
    "#         point[:, 0] = torch.Tensor([1 if x > 0.5 else 0 for x in point[:, 0]])\n",
    "        point[:, 0] = torch.Tensor(np.random.binomial(1,point[:, 0].data))\n",
    "        stroke[k] = point\n",
    "        point = point.unsqueeze(0)\n",
    "    return np.array(stroke.squeeze().data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stroke(stroke, save_name=None):\n",
    "    # Plot a single example.\n",
    "    f, ax = pyplot.subplots()\n",
    "\n",
    "    x = numpy.cumsum(stroke[:, 1])\n",
    "    y = numpy.cumsum(stroke[:, 2])\n",
    "\n",
    "    size_x = x.max() - x.min() + 1.\n",
    "    size_y = y.max() - y.min() + 1.\n",
    "\n",
    "    f.set_size_inches(5. * size_x / size_y, 5.)\n",
    "\n",
    "    cuts = numpy.where(stroke[:, 0] == 1)[0]\n",
    "    start = 0\n",
    "\n",
    "    for cut_value in cuts:\n",
    "        ax.plot(x[start:cut_value], y[start:cut_value],\n",
    "                'k-', linewidth=3)\n",
    "        start = cut_value + 1\n",
    "    ax.axis('equal')\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "    if save_name is None:\n",
    "        pyplot.show()\n",
    "    else:\n",
    "        try:\n",
    "            pyplot.savefig(\n",
    "                save_name,\n",
    "                bbox_inches='tight',\n",
    "                pad_inches=0.5)\n",
    "        except Exception:\n",
    "            print(\"Error building image!: \" + save_name)\n",
    "\n",
    "    pyplot.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stroke(stroke, save_name=None):\n",
    "    # Plot a single example.\n",
    "    f, ax = pyplot.subplots()\n",
    "\n",
    "    x = numpy.cumsum(stroke[:, 1])\n",
    "    y = numpy.cumsum(stroke[:, 2])\n",
    "\n",
    "    size_x = x.max() - x.min() + 1.\n",
    "    size_y = y.max() - y.min() + 1.\n",
    "\n",
    "    f.set_size_inches(5. * size_x / size_y, 5.)\n",
    "\n",
    "    cuts = numpy.where(stroke[:, 0] == 1)[0]\n",
    "    start = 0\n",
    "\n",
    "#     for cut_value in cuts:\n",
    "    ax.plot(x[start:cut_value], y[start:cut_value],\n",
    "            'k-', linewidth=3)\n",
    "#         start = cut_value + 1\n",
    "    ax.axis('equal')\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "    if save_name is None:\n",
    "        pyplot.show()\n",
    "    else:\n",
    "        try:\n",
    "            pyplot.savefig(\n",
    "                save_name,\n",
    "                bbox_inches='tight',\n",
    "                pad_inches=0.5)\n",
    "        except Exception:\n",
    "            print(\"Error building image!: \" + save_name)\n",
    "\n",
    "    pyplot.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "any(): argument 'dim' must be int, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-b6c3bd255e2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstroke\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_stroke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-123-a851157772e9>\u001b[0m in \u001b[0;36mgenerate_stroke\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#         point[:, 0] = torch.sigmoid(point[:, 0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#         point[:, 0] = torch.Tensor([1 if x > 0.5 else 0 for x in point[:, 0]])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mstroke\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.binomial\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36many\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   2162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m     \"\"\"\n\u001b[0;32m-> 2164\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_or\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'any'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: any(): argument 'dim' must be int, not NoneType"
     ]
    }
   ],
   "source": [
    "stroke = generate_stroke(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.8624933 ,  1.1118145 ],\n",
       "       [ 0.        ,  0.24259277,  0.35546863],\n",
       "       [ 0.        ,  0.31001937,  0.3623562 ],\n",
       "       ...,\n",
       "       [ 0.        ,  0.04025604, -0.46303684],\n",
       "       [ 0.        ,  0.0905305 , -0.38277978],\n",
       "       [ 0.        ,  0.12780105, -0.23997685]], dtype=float32)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stroke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = numpy.cumsum(stroke[:, 1])\n",
    "y = numpy.cumsum(stroke[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_x = x.max() - x.min() + 1.\n",
    "size_y = y.max() - y.min() + 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuts = numpy.where(stroke[:, 0] == 1)[0]\n",
    "start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = pyplot.subplots()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
